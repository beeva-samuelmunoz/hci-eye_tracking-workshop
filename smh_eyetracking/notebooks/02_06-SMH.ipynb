{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 SMH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See: http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.contrib import keras\n",
    "\n",
    "from smh_eyetracking.features02 import config as config_features02\n",
    "from smh_eyetracking.features02.utils.features02_dlib import FACE, JAWLINE, NOSE, LEFT_EYE, RIGHT_EYE, TARGETS\n",
    "from smh_eyetracking.keras import config as config_keras\n",
    "from smh_eyetracking.keras import losses\n",
    "from smh_eyetracking.utils import data_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, imgs_left, imgs_right = data_model.load(\n",
    "    config_features02.PATH_DATA_FEATURES02_DLIB_AUGMENTED_NORM_CSV,\n",
    "    config_features02.PATH_DATA_FEATURES02_DLIB_AUGMENTED_NORM_IMGS_LEFT,\n",
    "    config_features02.PATH_DATA_FEATURES02_DLIB_AUGMENTED_NORM_IMGS_RIGHT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    (train_data, train_imgs_left, train_imgs_right),\n",
    "    (validation_data, validation_imgs_left, validation_imgs_right),\n",
    "    (test_data, test_imgs_left, test_imgs_right)\n",
    ") = data_model.split(\n",
    "    data, imgs_left, imgs_right,\n",
    "    validation_size=0.15,\n",
    "    test_size=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train length: 65742\n",
      "Validation length: 383\n",
      "Test length: 447\n"
     ]
    }
   ],
   "source": [
    "print(\"Train length: {}\".format(len(train_data)))\n",
    "print(\"Validation length: {}\".format(len(validation_data)))\n",
    "print(\"Test length: {}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = config_features02.FEATURES02_EYES_SIZE\n",
    "img_shape = (img_height, img_width)\n",
    "\n",
    "def eye_prediction(imgs, eye_features, other_features):\n",
    "    c1 = keras.layers.Conv2D(\n",
    "        filters=16,\n",
    "        kernel_size=(5,5),\n",
    "        strides=(1,1),\n",
    "        padding=\"valid\",\n",
    "        activation='relu'\n",
    "    )(keras.layers.Reshape((24,32,1))(imgs))\n",
    "    s2 = keras.layers.AveragePooling2D(\n",
    "        pool_size=(2, 2),\n",
    "        strides=None,  # Same as pool_size\n",
    "        padding='valid'\n",
    "    )(c1)\n",
    "    c3 = keras.layers.Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=(3,3),\n",
    "        strides=(1,1),\n",
    "        padding=\"valid\",\n",
    "        activation='relu'\n",
    "    )(s2)\n",
    "    s4 = keras.layers.AveragePooling2D(\n",
    "        pool_size=(2, 2),\n",
    "        strides=None,  # Same as pool_size\n",
    "        padding='valid'\n",
    "    )(c3)\n",
    "    c5 = keras.layers.Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=(3,3),\n",
    "        strides=(1,1),\n",
    "        padding=\"valid\",\n",
    "        activation='relu'\n",
    "    )(s4)\n",
    "    f4 = keras.layers.Dense(128, activation=\"relu\")(keras.layers.Flatten()(c5))\n",
    "    #f6 = keras.layers.Dense(64, activation=\"relu\")(f4)\n",
    "    c7 = keras.layers.Concatenate()([f4, eye_features])\n",
    "    f8 = keras.layers.Dense(32, activation=\"relu\")(c7)\n",
    "    #f9 = keras.layers.Dense(32, activation=\"relu\")(f4)\n",
    "    return f8\n",
    "    \n",
    "\n",
    "def get_model():\n",
    "\n",
    "    # Inputs\n",
    "    left_imgs = keras.layers.Input(shape=img_shape, name='left_imgs', dtype='float32')\n",
    "    right_imgs = keras.layers.Input(shape=img_shape, name='right_imgs', dtype='float32')\n",
    "    \n",
    "    features_jawline = keras.layers.Input(shape=(len(JAWLINE),), name='features_jawline', dtype='float32')\n",
    "    features_nose = keras.layers.Input(shape=(len(NOSE),), name='features_nose', dtype='float32')\n",
    "    features_left_eye = keras.layers.Input(shape=(len(LEFT_EYE),), name='features_left_eye', dtype='float32')\n",
    "    features_right_eye = keras.layers.Input(shape=(len(RIGHT_EYE),), name='features_right_eye', dtype='float32')\n",
    "    features_face = keras.layers.Input(shape=(len(FACE),), name='features_face', dtype='float32')\n",
    "    \n",
    "    other_features = keras.layers.Concatenate()([features_jawline, features_nose, features_face])\n",
    "    \n",
    "\n",
    "    eye_predictions_x_01 = keras.layers.Dense(64, activation=\"relu\")(keras.layers.Concatenate()([\n",
    "        eye_prediction(left_imgs, features_left_eye, other_features),\n",
    "        eye_prediction(right_imgs, features_right_eye, other_features),\n",
    "        other_features\n",
    "    ]))\n",
    "    eye_predictions_x = keras.layers.Dense(1, activation=\"linear\")(eye_predictions_x_01)\n",
    "    \n",
    "\n",
    "    eye_predictions_y_01 = keras.layers.Dense(64, activation=\"relu\")(keras.layers.Concatenate()([\n",
    "        eye_prediction(left_imgs, features_left_eye, other_features),\n",
    "        eye_prediction(right_imgs, features_right_eye, other_features),\n",
    "        other_features\n",
    "    ]))\n",
    "    eye_predictions_y = keras.layers.Dense(1, activation=\"linear\")(eye_predictions_y_01)\n",
    "    \n",
    "    \n",
    "    eye_predictions = keras.layers.Concatenate()([eye_predictions_x,eye_predictions_y])\n",
    "   \n",
    "    # Model\n",
    "    model = keras.models.Model(\n",
    "        inputs=[\n",
    "            left_imgs, right_imgs,\n",
    "            features_jawline, features_nose, features_left_eye, features_right_eye, features_face\n",
    "        ],\n",
    "        outputs=[eye_predictions]\n",
    "    )\n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = '06_smh-01'\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.0005\n",
    "DECAY = 0.000001\n",
    "DROPOUT = 0\n",
    "\n",
    "LOSS = losses.mean_euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters to adjust: 390530\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    loss=LOSS,\n",
    "    metrics=[losses.mean_euclidean],\n",
    "    optimizer=keras.optimizers.Adam(lr=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=DECAY)\n",
    ")\n",
    "\n",
    "print(\"Parameters to adjust: {}\".format(\n",
    "    np.sum([keras.backend.count_params(p) for p in set(model.trainable_weights)])\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 65742 samples, validate on 383 samples\n",
      "Epoch 1/20\n",
      "65742/65742 [==============================] - 22s - loss: 0.3573 - mean_euclidean: 0.3573 - val_loss: 0.2864 - val_mean_euclidean: 0.2864\n",
      "Epoch 2/20\n",
      "65742/65742 [==============================] - 21s - loss: 0.2565 - mean_euclidean: 0.2565 - val_loss: 0.2334 - val_mean_euclidean: 0.2334\n",
      "Epoch 3/20\n",
      "65742/65742 [==============================] - 21s - loss: 0.2220 - mean_euclidean: 0.2220 - val_loss: 0.2323 - val_mean_euclidean: 0.2323\n",
      "Epoch 4/20\n",
      "65742/65742 [==============================] - 21s - loss: 0.2028 - mean_euclidean: 0.2028 - val_loss: 0.2067 - val_mean_euclidean: 0.2067\n",
      "Epoch 5/20\n",
      "65742/65742 [==============================] - 21s - loss: 0.1872 - mean_euclidean: 0.1872 - val_loss: 0.2037 - val_mean_euclidean: 0.2037\n",
      "Epoch 6/20\n",
      "65742/65742 [==============================] - 21s - loss: 0.1763 - mean_euclidean: 0.1763 - val_loss: 0.1967 - val_mean_euclidean: 0.1967\n",
      "Epoch 7/20\n",
      "65742/65742 [==============================] - 21s - loss: 0.1673 - mean_euclidean: 0.1673 - val_loss: 0.1988 - val_mean_euclidean: 0.1988\n",
      "Epoch 8/20\n",
      "65742/65742 [==============================] - 21s - loss: 0.1581 - mean_euclidean: 0.1581 - val_loss: 0.1985 - val_mean_euclidean: 0.1985\n",
      "Epoch 9/20\n",
      "65742/65742 [==============================] - 21s - loss: 0.1510 - mean_euclidean: 0.1510 - val_loss: 0.1882 - val_mean_euclidean: 0.1882\n",
      "Epoch 10/20\n",
      "65742/65742 [==============================] - 21s - loss: 0.1445 - mean_euclidean: 0.1445 - val_loss: 0.1891 - val_mean_euclidean: 0.1891\n",
      "Epoch 11/20\n",
      "65742/65742 [==============================] - 21s - loss: 0.1393 - mean_euclidean: 0.1393 - val_loss: 0.1961 - val_mean_euclidean: 0.1961\n",
      "Epoch 12/20\n",
      "65742/65742 [==============================] - 21s - loss: 0.1345 - mean_euclidean: 0.1345 - val_loss: 0.1898 - val_mean_euclidean: 0.1898\n",
      "Epoch 13/20\n",
      "65742/65742 [==============================] - 21s - loss: 0.1311 - mean_euclidean: 0.1311 - val_loss: 0.1993 - val_mean_euclidean: 0.1993\n",
      "Epoch 14/20\n",
      "65742/65742 [==============================] - 21s - loss: 0.1264 - mean_euclidean: 0.1264 - val_loss: 0.1987 - val_mean_euclidean: 0.1987\n",
      "Epoch 15/20\n",
      " 5248/65742 [=>............................] - ETA: 19s - loss: 0.1222 - mean_euclidean: 0.1222"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-7a7420c063c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m )\n",
      "\u001b[0;32m~/hci-eye_tracking-workshop/venv3.5/lib/python3.5/site-packages/tensorflow/contrib/keras/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\u001b[0m\n\u001b[1;32m   1493\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m         \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1495\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hci-eye_tracking-workshop/venv3.5/lib/python3.5/site-packages/tensorflow/contrib/keras/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1137\u001b[0m         \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m           \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hci-eye_tracking-workshop/venv3.5/lib/python3.5/site-packages/tensorflow/contrib/keras/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2286\u001b[0m       \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2287\u001b[0m     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2288\u001b[0;31m     \u001b[0mupdated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2289\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hci-eye_tracking-workshop/venv3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hci-eye_tracking-workshop/venv3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hci-eye_tracking-workshop/venv3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/hci-eye_tracking-workshop/venv3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hci-eye_tracking-workshop/venv3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    x={\n",
    "        'left_imgs':train_imgs_left,\n",
    "        'right_imgs': train_imgs_right,\n",
    "        'features_jawline': train_data[JAWLINE].as_matrix(),\n",
    "        'features_nose': train_data[NOSE].as_matrix(),\n",
    "        'features_left_eye': train_data[LEFT_EYE].as_matrix(),\n",
    "        'features_right_eye': train_data[RIGHT_EYE].as_matrix(),\n",
    "        'features_face': train_data[FACE].as_matrix()\n",
    "    },\n",
    "    y=train_data[TARGETS].as_matrix(),\n",
    "    validation_data=(\n",
    "        {\n",
    "            'left_imgs': validation_imgs_left,\n",
    "            'right_imgs': validation_imgs_right,\n",
    "            'features_jawline': validation_data[JAWLINE].as_matrix(),\n",
    "            'features_nose': validation_data[NOSE].as_matrix(),\n",
    "            'features_left_eye': validation_data[LEFT_EYE].as_matrix(),\n",
    "            'features_right_eye': validation_data[RIGHT_EYE].as_matrix(),\n",
    "            'features_face': validation_data[FACE].as_matrix()\n",
    "        },\n",
    "        validation_data[TARGETS].as_matrix()\n",
    "    ),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=1, callbacks=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(config_keras.PATH_MODELS_KERAS+MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = keras.models.load_model(\n",
    "    filepath=config_keras.PATH_MODELS_KERAS+MODEL_NAME,\n",
    "    custom_objects={\n",
    "        \"mean_euclidean\": losses.mean_euclidean,\n",
    "        \"ms_euclidean\": losses.ms_euclidean,\n",
    "        \"reg_mean_euclidean\": losses.reg_mean_euclidean\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.evaluate(\n",
    "    x={\n",
    "        'left_imgs':test_imgs_left,\n",
    "        'right_imgs': test_imgs_right,\n",
    "        'features_jawline': test_data[JAWLINE].as_matrix(),\n",
    "        'features_nose': test_data[NOSE].as_matrix(),\n",
    "        'features_left_eye': test_data[LEFT_EYE].as_matrix(),\n",
    "        'features_right_eye': test_data[RIGHT_EYE].as_matrix(),\n",
    "        'features_face': test_data[FACE].as_matrix()\n",
    "    },\n",
    "    y=test_data[TARGETS].as_matrix(),\n",
    "    batch_size=1,\n",
    "    verbose=1, sample_weight=None\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Corrected dataset\n",
    "\n",
    "\n",
    "| Name | Epochs | Batch Size | Learning rate | Decay | Loss | Train | Validation | Test |\n",
    "|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n",
    "| 05_lenet5-01 | 50 | 64| 0.0005 | 0.00001 | mean_euclidean |  |  |  |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
