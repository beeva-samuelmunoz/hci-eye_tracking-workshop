{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02_02 Twins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.contrib import keras\n",
    "\n",
    "from smh_eyetracking.features02 import config as config_features02\n",
    "from smh_eyetracking.features02.utils.features02_dlib import FEATURES, TARGETS\n",
    "from smh_eyetracking.keras import config as config_keras\n",
    "from smh_eyetracking.keras import losses\n",
    "from smh_eyetracking.utils import data_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, imgs_left, imgs_right = data_model.load(\n",
    "    config_features02.PATH_DATA_FEATURES02_DLIB_AUGMENTED_NORM_CSV,\n",
    "    config_features02.PATH_DATA_FEATURES02_DLIB_AUGMENTED_NORM_IMGS_LEFT,\n",
    "    config_features02.PATH_DATA_FEATURES02_DLIB_AUGMENTED_NORM_IMGS_RIGHT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    (train_data, train_imgs_left, train_imgs_right),\n",
    "    (validation_data, validation_imgs_left, validation_imgs_right),\n",
    "    (test_data, test_imgs_left, test_imgs_right)\n",
    ") = data_model.split(\n",
    "    data, imgs_left, imgs_right,\n",
    "    validation_size=0.15,\n",
    "    test_size=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Train length: {}\".format(len(train_data)))\n",
    "print(\"Validation length: {}\".format(len(validation_data)))\n",
    "print(\"Test length: {}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = config_features02.FEATURES02_EYES_SIZE\n",
    "img_shape = (img_height, img_width)\n",
    "\n",
    "def get_model():\n",
    "\n",
    "    # Inputs\n",
    "    left_imgs = keras.layers.Input(shape=img_shape, name='left_imgs', dtype='float32')\n",
    "    right_imgs = keras.layers.Input(shape=img_shape, name='right_imgs', dtype='float32')\n",
    "    features = keras.layers.Input(shape=(len(FEATURES),), name='features', dtype='float32')\n",
    "    \n",
    "    # Features\n",
    "    f1 = keras.layers.Dense(256, activation=keras.activations.relu)(features)\n",
    "    f2 = keras.layers.Dense(128, activation=keras.activations.relu)(f1)\n",
    "    f3 = keras.layers.Dense(64, activation=keras.activations.relu)(f2)    \n",
    "    \n",
    "    # Img left\n",
    "    l1 = keras.layers.Dense(1024, activation=keras.activations.relu)(keras.layers.Flatten()(left_imgs))\n",
    "    l2 = keras.layers.Dense(512, activation=keras.activations.relu)(l1)\n",
    "    l3 = keras.layers.Dense(128, activation=keras.activations.relu)(l2)\n",
    "    lf4 = keras.layers.Concatenate()([l3, f3])\n",
    "    l5 = keras.layers.Dense(64, activation=keras.activations.relu)(lf4)\n",
    "    \n",
    "    # Img Right\n",
    "    r1 = keras.layers.Dense(1024, activation=keras.activations.relu)(keras.layers.Flatten()(right_imgs))\n",
    "    r2 = keras.layers.Dense(512, activation=keras.activations.relu)(r1)\n",
    "    r3 = keras.layers.Dense(128, activation=keras.activations.relu)(r2)\n",
    "    rf4 = keras.layers.Concatenate()([r3, f3])\n",
    "    r5 = keras.layers.Dense(64, activation=keras.activations.relu)(rf4)\n",
    "    \n",
    "    # Output\n",
    "    o1 = keras.layers.Concatenate()([l5, r5])\n",
    "    o2 = keras.layers.Dense(64, activation=keras.activations.relu)(o1)\n",
    "    o3 = keras.layers.Dense(32, activation=keras.activations.relu)(o2)\n",
    "    o4 = keras.layers.Dense(2, activation=keras.activations.relu)(o3)\n",
    "\n",
    "    # Model\n",
    "    model = keras.models.Model(\n",
    "        inputs=[left_imgs, right_imgs, features],\n",
    "        outputs=[o4]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = '03_twins-01'\n",
    "\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.0005\n",
    "DECAY = 0.00001\n",
    "\n",
    "LOSS = losses.mean_euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    loss=LOSS,\n",
    "    metrics=[losses.mean_euclidean],\n",
    "    optimizer=keras.optimizers.Adam(lr=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=DECAY)\n",
    ")\n",
    "\n",
    "print(\"Parameters to adjust: {}\".format(\n",
    "    np.sum([keras.backend.count_params(p) for p in set(model.trainable_weights)])\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    x={\n",
    "        'left_imgs':train_imgs_left,\n",
    "        'right_imgs': train_imgs_right,\n",
    "        'features': train_data[FEATURES].as_matrix()\n",
    "    },\n",
    "    y=train_data[TARGETS].as_matrix(),\n",
    "    validation_data=(\n",
    "        {\n",
    "            'left_imgs': validation_imgs_left,\n",
    "            'right_imgs': validation_imgs_right,\n",
    "            'features': validation_data[FEATURES].as_matrix()\n",
    "        },\n",
    "        validation_data[TARGETS].as_matrix()\n",
    "    ),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=1, callbacks=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(config_keras.PATH_MODELS_KERAS+MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use model instead of model_test (loaded from file) since there is a bug in keras models.\n",
    "\n",
    "Issue: https://github.com/fchollet/keras/issues/3964\n",
    "\n",
    "```\n",
    "Optimizer weight shape (128,) not compatible with provided weight shape (256,)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.evaluate(\n",
    "    x={\n",
    "        'left_imgs':test_imgs_left,\n",
    "        'right_imgs': test_imgs_right,\n",
    "        'features': test_data[FEATURES].as_matrix()\n",
    "    },\n",
    "    y=test_data[TARGETS].as_matrix(),\n",
    "    batch_size=1,\n",
    "    verbose=1, sample_weight=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = keras.models.load_model(\n",
    "    filepath=config_keras.PATH_MODELS_KERAS+MODEL_NAME,\n",
    "    custom_objects={\n",
    "        \"mean_euclidean\": losses.mean_euclidean,\n",
    "        \"ms_euclidean\": losses.ms_euclidean,\n",
    "        \"reg_mean_euclidean\": losses.reg_mean_euclidean\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Corrected dataset and augmented (5 transformations)\n",
    "\n",
    "\n",
    "| Name | Epochs | Batch Size | Learning rate | Decay | Loss | Train | Validation | Test |\n",
    "|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n",
    "| f025baseline-01 | 150 | 128| 0.0005 | 0.00001 | mean_euclidean |  0.0209 | 0.1184 | 0.1201 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Corrected dataset\n",
    "\n",
    "\n",
    "| Name | Epochs | Batch Size | Learning rate | Decay | Loss | Train | Validation | Test |\n",
    "|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n",
    "| baseline-08a2 | 150 | 128| 0.0005 | 0.00001 | mean_euclidean | 0.0306 | 0.1286 | 0.1245 |\n",
    "| f02_baseline-09 | 150 | 128| 0.0005 | 0.00001 | mean_euclidean | 0.0384 | 0.1205 | 0.1182 |\n",
    "| f02_baseline-10 | 150 | 128| 0.0005 | 0.00001 | ms_euclidean | 0.0016 | 0.0234 | 0.02287 |\n",
    "| f02_baseline-13 | 150 | 128| 0.0005 | 0.00001 | mean_euclidean | 0.0381 | 0.1197 | 0.1199 |\n",
    "| f02_baseline-14 | 150 | 128| 0.0005 | 0.00001 | ms_euclidean | 0.0355 | 0.1176 |  |\n",
    "| f02_baseline-15 | 150 | 128| 0.0005 | 0.00001 | reg_mean_euclidean | 0.0425 | 0.1214 | 0.12201 |\n",
    "| f02_baseline-16 | 400 | 128| 0.0001 | 0.00001 | reg_mean_euclidean | 0.0215 | 0.1279 | 0.1287 |\n",
    "| f02_baseline-18x | 300 | 128| 0.0005 | 0.00001 | mean_euclidean | 0.0236 | 0.1123 | 0.1116 |"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Corrected dataset\n",
    "\n",
    "\n",
    "| Name | Epochs | Batch Size | Learning rate | Decay | Loss | Train | Validation | Test |\n",
    "|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n",
    "| baseline-08a2 | 150 | 128| 0.0005 | 0.00001 | mean_euclidean | 0.0306 | 0.1286 | 0.1245 |\n",
    "| f02_baseline-09 | 150 | 128| 0.0005 | 0.00001 | mean_euclidean | 0.0384 | 0.1205 | 0.1182 |\n",
    "| f02_baseline-10 | 150 | 128| 0.0005 | 0.00001 | ms_euclidean | 0.0016 | 0.0234 | 0.02287 |\n",
    "| f02_baseline-13 | 150 | 128| 0.0005 | 0.00001 | mean_euclidean | 0.0381 | 0.1197 | 0.1199 |\n",
    "| f02_baseline-14 | 150 | 128| 0.0005 | 0.00001 | ms_euclidean | 0.0355 | 0.1176 |  |\n",
    "| f02_baseline-15 | 150 | 128| 0.0005 | 0.00001 | reg_mean_euclidean | 0.0425 | 0.1214 | 0.12201 |\n",
    "| f02_baseline-16 | 400 | 128| 0.0001 | 0.00001 | reg_mean_euclidean | 0.0215 | 0.1279 | 0.1287 |\n",
    "| f02_baseline-18x | 300 | 128| 0.0005 | 0.00001 | mean_euclidean | 0.0236 | 0.1123 | 0.1116 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
