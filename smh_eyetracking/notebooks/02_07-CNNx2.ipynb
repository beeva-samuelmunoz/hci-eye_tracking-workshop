{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02_03 CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.contrib import keras\n",
    "\n",
    "from smh_eyetracking.features02 import config as config_features02\n",
    "from smh_eyetracking.features02.utils.features02_dlib import FEATURES, TARGETS\n",
    "from smh_eyetracking.keras import config as config_keras\n",
    "from smh_eyetracking.keras import losses\n",
    "from smh_eyetracking.utils import data_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, imgs_left, imgs_right = data_model.load(\n",
    "    config_features02.PATH_DATA_FEATURES02_DLIB_AUGMENTED_NORM_CSV,\n",
    "    config_features02.PATH_DATA_FEATURES02_DLIB_AUGMENTED_NORM_IMGS_LEFT,\n",
    "    config_features02.PATH_DATA_FEATURES02_DLIB_AUGMENTED_NORM_IMGS_RIGHT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    (train_data, train_imgs_left, train_imgs_right),\n",
    "    (validation_data, validation_imgs_left, validation_imgs_right),\n",
    "    (test_data, test_imgs_left, test_imgs_right)\n",
    ") = data_model.split(\n",
    "    data, imgs_left, imgs_right,\n",
    "    validation_size=0.15,\n",
    "    test_size=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Train length: {}\".format(len(train_data)))\n",
    "print(\"Validation length: {}\".format(len(validation_data)))\n",
    "print(\"Test length: {}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = config_features02.FEATURES02_EYES_SIZE\n",
    "img_shape = (img_height, img_width)\n",
    "\n",
    "def get_model():\n",
    "\n",
    "    # Inputs\n",
    "    left_imgs = keras.layers.Input(shape=img_shape, name='left_imgs', dtype='float32')\n",
    "    right_imgs = keras.layers.Input(shape=img_shape, name='right_imgs', dtype='float32')\n",
    "    features = keras.layers.Input(shape=(len(FEATURES),), name='features', dtype='float32')\n",
    "    \n",
    "    \n",
    "    # Model X\n",
    "\n",
    "    # Features\n",
    "    xf1 = keras.layers.Dense(256, activation=keras.activations.relu)(features)\n",
    "    xf2 = keras.layers.Dense(128, activation=keras.activations.relu)(xf1)\n",
    "    # Left imgs\n",
    "    xl1 = keras.layers.Conv2D(32, (3, 3), (2,2), activation='relu', padding='same')(\n",
    "        keras.layers.Reshape((24,32,1))(left_imgs)\n",
    "    )\n",
    "    xl2 = keras.layers.Conv2D(64, (3, 3), (2,2), activation='relu', padding='same')(xl1)\n",
    "    # Right imgs\n",
    "    xr1 = keras.layers.Conv2D(32, (3, 3), (2,2), activation='relu', padding='same')(\n",
    "        keras.layers.Reshape((24,32,1))(right_imgs)\n",
    "    )\n",
    "    xr2 = keras.layers.Conv2D(64, (3, 3), (2,2), activation='relu', padding='same', input_shape=(24, 32, 1))(xr1)\n",
    "    # Concatenate\n",
    "    xconcat = keras.layers.Concatenate()([\n",
    "        keras.layers.Flatten()(xl2),\n",
    "        keras.layers.Flatten()(xr2),\n",
    "        xf2\n",
    "    ])\n",
    "    # Dense\n",
    "    xd_1 = keras.layers.Dense(256, activation=keras.activations.relu)(xconcat)\n",
    "    xd_2 = keras.layers.Dense(128, activation=keras.activations.relu)(xd_1)\n",
    "    xd_3 = keras.layers.Dense(32, activation=keras.activations.relu)(xd_2)\n",
    "    xd_4 = keras.layers.Dense(1)(xd_3)\n",
    "    \n",
    "     \n",
    "    # Model Y\n",
    "    # Features\n",
    "    yf1 = keras.layers.Dense(256, activation=keras.activations.relu)(features)\n",
    "    yf2 = keras.layers.Dense(128, activation=keras.activations.relu)(yf1)\n",
    "    # Left imgs\n",
    "    yl1 = keras.layers.Conv2D(32, (3, 3), (2,2), activation='relu', padding='same')(\n",
    "        keras.layers.Reshape((24,32,1))(left_imgs)\n",
    "    )\n",
    "    yl2 = keras.layers.Conv2D(64, (3, 3), (2,2), activation='relu', padding='same')(yl1)\n",
    "    # Right imgs\n",
    "    yr1 = keras.layers.Conv2D(32, (3, 3), (2,2), activation='relu', padding='same')(\n",
    "        keras.layers.Reshape((24,32,1))(right_imgs)\n",
    "    )\n",
    "    yr2 = keras.layers.Conv2D(64, (3, 3), (2,2), activation='relu', padding='same', input_shape=(24, 32, 1))(yr1)\n",
    "    # Concatenate\n",
    "    yconcat = keras.layers.Concatenate()([\n",
    "        keras.layers.Flatten()(yl2),\n",
    "        keras.layers.Flatten()(yr2),\n",
    "        yf2\n",
    "    ])\n",
    "    \n",
    "    # Dense\n",
    "    yd_1 = keras.layers.Dense(256, activation=keras.activations.relu)(yconcat)\n",
    "    yd_2 = keras.layers.Dense(128, activation=keras.activations.relu)(yd_1)\n",
    "    yd_3 = keras.layers.Dense(32, activation=keras.activations.relu)(yd_2)\n",
    "    yd_4 = keras.layers.Dense(1)(yd_3)\n",
    "    \n",
    "    \n",
    "    out = keras.layers.Concatenate()([xd_4, yd_4])\n",
    "    # Model\n",
    "    model = keras.models.Model(\n",
    "        inputs=[left_imgs, right_imgs, features],\n",
    "        outputs=[out]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = '07-CNNx2-01'\n",
    "\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.00005\n",
    "DECAY = 0.000001\n",
    "\n",
    "\n",
    "LOSS = losses.mean_euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    loss=LOSS,\n",
    "    metrics=[losses.mean_euclidean],\n",
    "    optimizer=keras.optimizers.Adam(lr=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=DECAY)\n",
    ")\n",
    "\n",
    "print(\"Parameters to adjust: {}\".format(\n",
    "    np.sum([keras.backend.count_params(p) for p in set(model.trainable_weights)])\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    x={\n",
    "        'left_imgs':train_imgs_left,\n",
    "        'right_imgs': train_imgs_right,\n",
    "        'features': train_data[FEATURES].as_matrix()\n",
    "    },\n",
    "    y=train_data[TARGETS].as_matrix(),\n",
    "    validation_data=(\n",
    "        {\n",
    "            'left_imgs': validation_imgs_left,\n",
    "            'right_imgs': validation_imgs_right,\n",
    "            'features': validation_data[FEATURES].as_matrix()\n",
    "        },\n",
    "        validation_data[TARGETS].as_matrix()\n",
    "    ),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=1, callbacks=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(config_keras.PATH_MODELS_KERAS+MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = keras.models.load_model(\n",
    "    filepath=config_keras.PATH_MODELS_KERAS+MODEL_NAME,\n",
    "    custom_objects={\n",
    "        \"mean_euclidean\": losses.mean_euclidean,\n",
    "        \"ms_euclidean\": losses.ms_euclidean,\n",
    "        \"reg_mean_euclidean\": losses.reg_mean_euclidean\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.evaluate(\n",
    "    x={\n",
    "        'left_imgs':test_imgs_left,\n",
    "        'right_imgs': test_imgs_right,\n",
    "        'features': test_data[FEATURES].as_matrix()\n",
    "    },\n",
    "    y=test_data[TARGETS].as_matrix(),\n",
    "    batch_size=1,\n",
    "    verbose=1, sample_weight=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Corrected dataset\n",
    "\n",
    "\n",
    "| Name | Epochs | Batch Size | Learning rate | Decay | Loss | Train | Validation | Test |\n",
    "|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n",
    "| 07_CNNx2-01 | 50 | 64| 0.00005 |0.000001 | mean_euclidean | 0.0399 | 0.1807 | 0.1676 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
